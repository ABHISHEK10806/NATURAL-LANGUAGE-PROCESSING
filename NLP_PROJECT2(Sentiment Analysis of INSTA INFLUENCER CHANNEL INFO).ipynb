{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfpqhfcWLkMITwMNX5c6+5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LOAD DATASET\n"],"metadata":{"id":"VUjSsw5kB6gh"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"/content/top_insta_influencers_data.csv\")\n","\n","df = df[['channel_info']].dropna()\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SnMByg4w_Jwe","executionInfo":{"status":"ok","timestamp":1767026977411,"user_tz":-330,"elapsed":19,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}},"outputId":"baeae60b-4c12-4be4-ee54-7ddc2a4654d0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["  channel_info\n","0    cristiano\n","1  kyliejenner\n","2     leomessi\n","3  selenagomez\n","4      therock\n"]}]},{"cell_type":"markdown","source":["# CREATE SENTIMENT"],"metadata":{"id":"ugrgRjDNCAh_"}},{"cell_type":"code","source":["positive_words = [\n","    'official', 'fitness', 'love', 'music', 'actor',\n","    'cricket', 'football', 'fashion', 'model', 'star'\n","]\n","\n","def get_sentiment(text):\n","    text = str(text).lower()\n","    for word in positive_words:\n","        if word in text:\n","            return 1   # Positive\n","    return 0           # Negative\n","\n","df['sentiment'] = df['channel_info'].apply(get_sentiment)"],"metadata":{"id":"5bM8Nmqr_seO","executionInfo":{"status":"ok","timestamp":1767027259288,"user_tz":-330,"elapsed":68,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# TOKENIZATION"],"metadata":{"id":"RqPu17ZuCF80"}},{"cell_type":"code","source":["def tokenize(text):\n","    return str(text).lower().split()\n",""],"metadata":{"id":"pDAyL1JECDXf","executionInfo":{"status":"ok","timestamp":1767027312963,"user_tz":-330,"elapsed":7,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# MANUAL VOCABULARY"],"metadata":{"id":"UNrIqhB2CWEL"}},{"cell_type":"code","source":["word2idx = {\"<pad>\": 0}\n","idx = 1\n","\n","for text in df['channel_info']:\n","    for word in tokenize(text):\n","        if word not in word2idx:\n","            word2idx[word] = idx\n","            idx += 1\n","\n","vocab_size = len(word2idx)\n"],"metadata":{"id":"A8mXC3I2CQeM","executionInfo":{"status":"ok","timestamp":1767027354953,"user_tz":-330,"elapsed":13,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# ENCODE + PADDING"],"metadata":{"id":"5S2DWStKCeAD"}},{"cell_type":"code","source":["import torch\n","\n","MAX_LEN = 20\n","\n","def encode(text):\n","    tokens = tokenize(text)\n","    encoded = [word2idx.get(word, 0) for word in tokens]\n","    encoded = encoded[:MAX_LEN]\n","    encoded += [0] * (MAX_LEN - len(encoded))\n","    return torch.tensor(encoded)\n"],"metadata":{"id":"X9ziT-PeCatp","executionInfo":{"status":"ok","timestamp":1767027387690,"user_tz":-330,"elapsed":69,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# DATASET CLASS"],"metadata":{"id":"qw78CWoJClBz"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class InstaSentimentDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return encode(self.texts[idx]), torch.tensor(self.labels[idx], dtype=torch.float32)\n"],"metadata":{"id":"2czqCbHMCitz","executionInfo":{"status":"ok","timestamp":1767027409641,"user_tz":-330,"elapsed":3,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["# TRAIN-TEST SPLIT"],"metadata":{"id":"urG13aC6CqNI"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['channel_info'], df['sentiment'], test_size=0.2, random_state=42\n",")\n","\n","train_data = InstaSentimentDataset(X_train.values, y_train.values)\n","train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n"],"metadata":{"id":"XOKDYLUhCoFj","executionInfo":{"status":"ok","timestamp":1767027442806,"user_tz":-330,"elapsed":52,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# SIMPLE ANN MODEL"],"metadata":{"id":"TRaP7o83C4IY"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class SentimentModel(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, 32)\n","        self.fc = nn.Linear(32 * MAX_LEN, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.view(x.size(0), -1)\n","        return self.sigmoid(self.fc(x))\n"],"metadata":{"id":"fHe2LkqfCwKu","executionInfo":{"status":"ok","timestamp":1767027491147,"user_tz":-330,"elapsed":16,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# TRAINING"],"metadata":{"id":"yA-TfmocC-O6"}},{"cell_type":"code","source":["model = SentimentModel(vocab_size)\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(3):\n","    total_loss = 0\n","    for text, label in train_loader:\n","        optimizer.zero_grad()\n","        output = model(text).squeeze()\n","        loss = criterion(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VjmZrM5BC7-f","executionInfo":{"status":"ok","timestamp":1767027512970,"user_tz":-330,"elapsed":4731,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}},"outputId":"bba3a75e-7294-420e-9bca-32d1a4de98bd"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 2.1316\n","Epoch 2, Loss: 0.7103\n","Epoch 3, Loss: 0.7638\n"]}]},{"cell_type":"markdown","source":["# PREDICTION"],"metadata":{"id":"nMOXJ43_DJ0l"}},{"cell_type":"code","source":["def predict_sentiment(text):\n","    model.eval()\n","    with torch.no_grad():\n","        encoded = encode(text).unsqueeze(0)\n","        output = model(encoded)\n","        return \"Positive ðŸ˜Š\" if output.item() > 0.5 else \"Negative ðŸ˜ž\"\n","\n","print(predict_sentiment(\"official fitness model\"))\n","print(predict_sentiment(\"random channel page\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1WpdHRoDAJl","executionInfo":{"status":"ok","timestamp":1767027558327,"user_tz":-330,"elapsed":21,"user":{"displayName":"ABHISHEK","userId":"10144540838703416989"}},"outputId":"52893d81-df8f-405d-bc6e-b1580e8ea65e"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Negative ðŸ˜ž\n","Negative ðŸ˜ž\n"]}]}]}